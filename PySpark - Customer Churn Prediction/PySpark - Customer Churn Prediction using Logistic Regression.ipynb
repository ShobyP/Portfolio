{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading PySpark supporting libraries.\n",
    "\n",
    "import findspark\n",
    "findspark.init('/home/shoby/spark-2.4.0-bin-hadoop2.7')\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Spark App\n",
    "\n",
    "spark = SparkSession.builder.appName('Customer_Churn').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset, custome\n",
    "\n",
    "data = spark.read.csv('customer_churn.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n",
      "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n",
      "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking how data looks like.\n",
    "\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Company)|\n",
      "+-----------------------+\n",
      "|                    873|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Importing countDistinct to count unique values.\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "#Checking how many distinct Companies are there.\n",
    "\n",
    "data.agg(countDistinct('Company')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data processing plan.\n",
    "\n",
    "#1 - Drop [na, Names, Account_Manager, Company]\n",
    "#2 - Extract onboard_date year\n",
    "#3 - Extract location city\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - Dropping Na.\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - Dropping Names, Account_Manager\n",
    "\n",
    "data = data.drop('Names','Account_Manager','Company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|Location                                          |\n",
      "+--------------------------------------------------+\n",
      "|10265 Elizabeth Mission Barkerburgh, AK 89518     |\n",
      "|6157 Frank Gardens Suite 019 Carloshaven, RI 17756|\n",
      "|1331 Keith Court Alyssahaven, DE 90114            |\n",
      "+--------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the location data and see if its usefull for our use.\n",
    "# its not.\n",
    "data.select('Location').show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dropping location column.\n",
    "\n",
    "data = data.drop('Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----+---------+-------------------+-----+\n",
      "| Age|Total_Purchase|Years|Num_Sites|       Onboard_date|Churn|\n",
      "+----+--------------+-----+---------+-------------------+-----+\n",
      "|42.0|       11066.8| 7.22|      8.0|2013-08-30 07:00:40|    1|\n",
      "|41.0|      11916.22|  6.5|     11.0|2013-08-13 00:38:46|    1|\n",
      "|38.0|      12884.75| 6.67|     12.0|2016-06-29 06:20:07|    1|\n",
      "+----+--------------+-----+---------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing functions to extract timestamp data.\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addting years column to the dataframe, extracting data from Onboard_date\n",
    "\n",
    "data = data.withColumn('years', year(\"Onboard_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addting months column to the dataframe, extracting data from Onboard_date\n",
    "\n",
    "data = data.withColumn('months', month(\"Onboard_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addting dayofmonth column to the dataframe, extracting data from Onboard_date\n",
    "\n",
    "data = data.withColumn('DayOfMonth', dayofmonth(\"Onboard_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----+---------+-------------------+-----+------+----------+\n",
      "| Age|Total_Purchase|years|Num_Sites|       Onboard_date|Churn|months|DayOfMonth|\n",
      "+----+--------------+-----+---------+-------------------+-----+------+----------+\n",
      "|42.0|       11066.8| 2013|      8.0|2013-08-30 07:00:40|    1|     8|        30|\n",
      "|41.0|      11916.22| 2013|     11.0|2013-08-13 00:38:46|    1|     8|        13|\n",
      "|38.0|      12884.75| 2016|     12.0|2016-06-29 06:20:07|    1|     6|        29|\n",
      "+----+--------------+-----+---------+-------------------+-----+------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Onboard_date column as its data has been extracted.\n",
    "\n",
    "data = data.drop('Onboard_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing vectrizer so that data can be vectored.\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'Total_Purchase',\n",
       " 'years',\n",
       " 'Num_Sites',\n",
       " 'Churn',\n",
       " 'months',\n",
       " 'DayOfMonth']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking data column names, so vectorAssembler can be configured.\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a VectorAssembler instance and configuring it for input and output columns.\n",
    "\n",
    "VA = VectorAssembler(inputCols=['Age', 'Total_Purchase', 'years', 'Num_Sites', 'months', 'DayOfMonth'], \n",
    "                        outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the data using VectorAssembler, into vData.\n",
    "\n",
    "vData = VA.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----+---------+-----+------+----------+--------------------+\n",
      "| Age|Total_Purchase|years|Num_Sites|Churn|months|DayOfMonth|            features|\n",
      "+----+--------------+-----+---------+-----+------+----------+--------------------+\n",
      "|42.0|       11066.8| 2013|      8.0|    1|     8|        30|[42.0,11066.8,201...|\n",
      "|41.0|      11916.22| 2013|     11.0|    1|     8|        13|[41.0,11916.22,20...|\n",
      "|38.0|      12884.75| 2016|     12.0|    1|     6|        29|[38.0,12884.75,20...|\n",
      "+----+--------------+-----+---------+-----+------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking how vData looks like.\n",
    "\n",
    "vData.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a final dataset for test and training.\n",
    "\n",
    "final_data = vData.select('features', 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|Churn|\n",
      "+--------------------+-----+\n",
      "|[42.0,11066.8,201...|    1|\n",
      "|[41.0,11916.22,20...|    1|\n",
      "|[38.0,12884.75,20...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking how final_data looks\n",
    "\n",
    "final_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting final data into test and train data.\n",
    "\n",
    "train_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              Churn|\n",
      "+-------+-------------------+\n",
      "|  count|                598|\n",
      "|   mean|0.18561872909698995|\n",
      "| stddev|0.38912417456799425|\n",
      "|    min|                  0|\n",
      "|    max|                  1|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking test and train data.\n",
    "\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              Churn|\n",
      "+-------+-------------------+\n",
      "|  count|                302|\n",
      "|   mean| 0.1291390728476821|\n",
      "| stddev|0.33591040649627546|\n",
      "|    min|                  0|\n",
      "|    max|                  1|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing logistic regression.\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a logistic regression instance.\n",
    "#Setting up logistic regression algorithm for incoming data.\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='Churn', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating logistic regression model based on above specification and final_data\n",
    "\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating test_data using Lr model.\n",
    "\n",
    "lr_predictions = lr_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|Churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[28.0,11245.38,20...|    0|[3.98004270550838...|[0.98165787826014...|       0.0|\n",
      "|[29.0,5900.78,200...|    0|[3.55353209193736...|[0.97217313775616...|       0.0|\n",
      "|[29.0,8688.17,201...|    1|[3.04328803886282...|[0.95449186585591...|       0.0|\n",
      "|[29.0,9617.59,201...|    0|[3.72597389458189...|[0.97647703132984...|       0.0|\n",
      "|[30.0,6744.87,201...|    0|[2.78658894142940...|[0.94194679866292...|       0.0|\n",
      "|[30.0,7960.64,201...|    1|[1.54775501253021...|[0.82458924935051...|       0.0|\n",
      "|[30.0,8874.83,200...|    0|[2.64308566535218...|[0.93358354821823...|       0.0|\n",
      "|[30.0,12788.37,20...|    0|[1.28285293796858...|[0.78293501824053...|       0.0|\n",
      "|[30.0,13473.35,20...|    0|[1.77199083959460...|[0.85470507672800...|       0.0|\n",
      "|[31.0,8688.21,200...|    0|[4.98638218929876...|[0.99321600623086...|       0.0|\n",
      "|[31.0,10058.87,20...|    0|[5.09181944559752...|[0.99389072674749...|       0.0|\n",
      "|[31.0,11743.24,20...|    0|[6.18982576614658...|[0.99795401072074...|       0.0|\n",
      "|[31.0,12264.68,20...|    0|[3.82235165033571...|[0.97859203206147...|       0.0|\n",
      "|[32.0,6367.22,201...|    0|[1.82877078977523...|[0.86161522778808...|       0.0|\n",
      "|[32.0,9036.27,201...|    0|[-0.1482040254000...|[0.46301666205338...|       1.0|\n",
      "|[32.0,9472.72,200...|    0|[1.09406505706041...|[0.74914642519377...|       0.0|\n",
      "|[32.0,9885.12,200...|    1|[2.70756467488728...|[0.93747154515459...|       0.0|\n",
      "|[32.0,12142.99,20...|    0|[5.09722080877887...|[0.99392343621681...|       0.0|\n",
      "|[33.0,4711.89,200...|    0|[4.30980020525468...|[0.98674190504870...|       0.0|\n",
      "|[33.0,7492.9,2009...|    0|[4.65844743368830...|[0.99060787719604...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predictions.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing classification evaluator.\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a binary classification evaluator item wheren rawPredictions are the \"prediction\" col\n",
    "#Actual labels column is 'Churn' column. we will use this classifier to find accuracy of prediction.\n",
    "\n",
    "churn_eval = BinaryClassificationEvaluator(rawPredictionCol = 'prediction', labelCol = 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding area under curve by evaluating binary classification on lr prediction's predictions.\n",
    "\n",
    "auc = churn_eval.evaluate(lr_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.748318221702252"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Area under curve is 77.4%\n",
    "\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on above data pre-processing sequence, here is a function that takes new customer data and transform\n",
    "#it for model compatability.\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def dataProcessing(dataframe):\n",
    "    dataframe = dataframe.dropna()\n",
    "    dataframe = dataframe.drop('Names','Account_Manager','Location')\n",
    "    dataframe = dataframe.withColumn('years', year(\"Onboard_date\"))\n",
    "    dataframe = dataframe.withColumn('months', month(\"Onboard_date\"))\n",
    "    dataframe = dataframe.withColumn('DayOfMonth', dayofmonth(\"Onboard_date\"))\n",
    "    dataframe = dataframe.drop('Onboard_date')\n",
    "    VA = VectorAssembler(inputCols=['Age', 'Total_Purchase', 'years', 'Num_Sites', 'months', 'DayOfMonth'], \n",
    "                        outputCol='features')\n",
    "    vData = VA.transform(dataframe)\n",
    "    return vData\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing production (simulation) customer data.\n",
    "\n",
    "new_data = spark.read.csv('new_customers.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n",
      "|         Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|         Company|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n",
      "| Andrew Mccall|37.0|       9935.53|              1| 7.71|      8.0|2011-08-29 18:37:54|38612 Johnny Stra...|        King Ltd|\n",
      "|Michele Wright|23.0|       7526.94|              1| 9.28|     15.0|2013-07-22 18:19:54|21083 Nicole Junc...|   Cannon-Benson|\n",
      "|  Jeremy Chang|65.0|         100.0|              1|  1.0|     15.0|2006-12-11 07:48:13|085 Austin Views ...|Barron-Robertson|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Seeing how the data looks like.\n",
    "\n",
    "new_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running dataProcessing function on new_cust data.\n",
    "\n",
    "new_cust = dataProcessing(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----+---------+----------------+------+----------+--------------------+\n",
      "| Age|Total_Purchase|years|Num_Sites|         Company|months|DayOfMonth|            features|\n",
      "+----+--------------+-----+---------+----------------+------+----------+--------------------+\n",
      "|37.0|       9935.53| 2011|      8.0|        King Ltd|     8|        29|[37.0,9935.53,201...|\n",
      "|23.0|       7526.94| 2013|     15.0|   Cannon-Benson|     7|        22|[23.0,7526.94,201...|\n",
      "|65.0|         100.0| 2006|     15.0|Barron-Robertson|    12|        11|[65.0,100.0,2006....|\n",
      "+----+--------------+-----+---------+----------------+------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking new customer output after function application.\n",
    "\n",
    "new_cust.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using lr_model to transform new_cust data.\n",
    "\n",
    "new_trans = lr_model.transform(new_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|         Company|prediction|\n",
      "+----------------+----------+\n",
      "|        King Ltd|       0.0|\n",
      "|   Cannon-Benson|       1.0|\n",
      "|Barron-Robertson|       1.0|\n",
      "|   Sexton-Golden|       1.0|\n",
      "|        Wood LLC|       0.0|\n",
      "|   Parks-Robbins|       1.0|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Final predictions using deployed model on production dataset. \n",
    "\n",
    "final_prediction = new_trans.select('Company','prediction').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
